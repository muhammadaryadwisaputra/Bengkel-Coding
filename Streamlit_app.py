# -*- coding: utf-8 -*-
"""Uas(BK)ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PFKN3b5Rfu6hgQv6PDr1qQhgc-yw8Lgv

# Exploratory Data Analysis
"""

# 1. Mount Google Drive (jalankan ini dulu)
from google.colab import drive
drive.mount('/content/drive')

# 2. Import library
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# 3. Load dataset (ganti path ini dengan yang benar sesuai lokasi file di Drive kamu)
df = pd.read_csv('/content/drive/MyDrive/DataBK/ObesityDataSet.csv')

# 4 Tampilkan beberapa baris pertama
print("5 Baris Pertama Dataset:")
print(df.head())

# 5. Informasi Umum Dataset
print("\nInformasi Dataset:")
print(df.info())

print("\nDeskripsi Statistik:")
print(df.describe(include='all'))

# 6. Cek Missing Values
print("\nJumlah Missing Values per Kolom:")
print(df.isnull().sum())

# 7. Cek Unique Values
print("\nJumlah Nilai Unik per Kolom:")
print(df.nunique())

# 8. Cek Duplikasi
print("\nJumlah Data Duplikat:")
print(df.duplicated().sum())

# 9. Visualisasi Distribusi Target
plt.figure(figsize=(10,5))
sns.countplot(data=df, x='NObeyesdad', order=df['NObeyesdad'].value_counts().index)
plt.title("Distribusi Target (Obesity Level)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Daftar fitur numerik
numerical_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']

# Loop untuk setiap fitur
for feature in numerical_features:
    plt.figure(figsize=(14, 5))

    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[feature], kde=True, color='skyblue')
    plt.title(f'Distribusi dari {feature}', fontsize=12)
    plt.xlabel(feature)
    plt.ylabel('Jumlah')

    # Boxplot
    plt.subplot(1, 2, 2)
    sns.boxplot(x=df[feature], color='orange')
    plt.title(f'Boxplot dari {feature}', fontsize=12)
    plt.xlabel(feature)

    plt.tight_layout()
    plt.show()

"""# Prosesing data


"""

# Cek missing values
print("Jumlah Missing Values per Kolom:")
print(df.isnull().sum())

# Cek duplikasi
print("\nJumlah Data Duplikat:", df.duplicated().sum())

# Hapus duplikasi (jika ada)
df = df.drop_duplicates()

# Pastikan df bukan slice dari DataFrame lain (gunakan .copy() jika hasil filter sebelumnya)
df = df.copy()

# Konversi kolom numerik ke float
numerik_cols = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']
for col in numerik_cols:
    df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')  # menggunakan .loc agar aman

# Hapus baris yang memiliki nilai NaN
df.dropna(inplace=True)

def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return data[(data[column] >= lower) & (data[column] <= upper)]

# Terapkan pada beberapa kolom penting
for col in ['Age', 'Height', 'Weight']:
    df = remove_outliers_iqr(df, col)

from sklearn.preprocessing import LabelEncoder

categorical_cols = df.select_dtypes(include='object').columns
le = LabelEncoder()

for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

from imblearn.over_sampling import SMOTE

X = df.drop('NObeyesdad', axis=1)
y = df['NObeyesdad']

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

X_final = pd.DataFrame(X_scaled, columns=X.columns)
y_final = y_resampled.reset_index(drop=True)

print("Shape data akhir (fitur):", X_final.shape)
print("Distribusi target setelah SMOTE:\n", y_final.value_counts())

"""Kesimpulan Preprocessing
Missing values dan data duplikat telah ditangani.

Outlier telah diatasi dengan metode IQR (misalnya pada 'Weight').

Semua fitur kategorikal telah dikonversi menjadi numerik.

Seluruh fitur masih digunakan, bisa difilter lebih lanjut pada tahap modeling.

Ketidakseimbangan kelas target telah diatasi menggunakan SMOTE.

Fitur numerik telah distandardisasi menggunakan StandardScaler.

# Pemodelan dan Evaluasi
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

models = {
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier(random_state=42)
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, average='weighted')
    rec = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    results[name] = {
        "Accuracy": acc,
        "Precision": prec,
        "Recall": rec,
        "F1 Score": f1
    }

    print(f"\n{name}:\n")
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import pandas as pd

# Convert results ke DataFrame
df_results = pd.DataFrame(results).T

df_results.plot(kind='bar', figsize=(10, 6))
plt.title('Perbandingan Performa Model')
plt.ylabel('Score')
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.legend(loc='lower right')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""# Hyperparameter Tuning"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# ------------------- SCALING -------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ------------------- MODEL SEBELUM TUNING -------------------
model_knn_before = KNeighborsClassifier()
model_svm_before = SVC()
model_rf_before = RandomForestClassifier()

model_knn_before.fit(X_train_scaled, y_train)
model_svm_before.fit(X_train_scaled, y_train)
model_rf_before.fit(X_train_scaled, y_train)

acc_knn_before = accuracy_score(y_test, model_knn_before.predict(X_test_scaled))
acc_svm_before = accuracy_score(y_test, model_svm_before.predict(X_test_scaled))
acc_rf_before = accuracy_score(y_test, model_rf_before.predict(X_test_scaled))

# ------------------- HYPERPARAMETER TUNING -------------------
# KNN
param_knn = {'n_neighbors': [3, 5, 7, 9]}
grid_knn = GridSearchCV(KNeighborsClassifier(), param_knn, cv=5)
grid_knn.fit(X_train_scaled, y_train)
best_knn = grid_knn.best_estimator_

# SVM
param_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid_svm = GridSearchCV(SVC(), param_svm, cv=5)
grid_svm.fit(X_train_scaled, y_train)
best_svm = grid_svm.best_estimator_

# Random Forest
param_rf = {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}
grid_rf = GridSearchCV(RandomForestClassifier(), param_rf, cv=5)
grid_rf.fit(X_train_scaled, y_train)
best_rf = grid_rf.best_estimator_

# ------------------- AKURASI SETELAH TUNING -------------------
acc_knn_after = accuracy_score(y_test, best_knn.predict(X_test_scaled))
acc_svm_after = accuracy_score(y_test, best_svm.predict(X_test_scaled))
acc_rf_after = accuracy_score(y_test, best_rf.predict(X_test_scaled))

print("\n=== Akurasi Setelah Hyperparameter Tuning ===")
print(f"KNN Accuracy: {acc_knn_after:.4f}")
print(f"SVM Accuracy: {acc_svm_after:.4f}")
print(f"Random Forest Accuracy: {acc_rf_after:.4f}")

# ------------------- VISUALISASI PERBANDINGAN -------------------
model_names = ['KNN', 'SVM', 'Random Forest']
acc_before = [acc_knn_before, acc_svm_before, acc_rf_before]
acc_after = [acc_knn_after, acc_svm_after, acc_rf_after]

x = range(len(model_names))
plt.bar(x, acc_before, width=0.4, label='Sebelum Tuning', align='center')
plt.bar([i + 0.4 for i in x], acc_after, width=0.4, label='Sesudah Tuning', align='center')
plt.xticks([i + 0.2 for i in x], model_names)
plt.ylabel('Akurasi')
plt.title('Perbandingan Akurasi Sebelum & Sesudah Hyperparameter Tuning')
plt.legend()
plt.show()

# ------------------- HEATMAP CONFUSION MATRIX -------------------
models_after = {
    "KNN (Tuned)": best_knn,
    "SVM (Tuned)": best_svm,
    "Random Forest (Tuned)": best_rf
}

for name, model in models_after.items():
    y_pred = model.predict(X_test_scaled)
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()
    print(f"\nClassification Report for {name}:\n")
    print(classification_report(y_test, y_pred))
    # ------------------- SIMPAN MODEL TERBAIK -------------------
# Ganti ke best_rf atau best_knn jika lebih bagus
best_model = best_rf
joblib.dump(best_model, 'best_model.pkl')
print("Model terbaik berhasil disimpan sebagai 'best_model.pkl'")

"""# Bagian Baru"""


import streamlit as st
import joblib
import numpy as np

# Load model
model = joblib.load('best_model.pkl')

# Judul aplikasi
st.title("ðŸŽ¯ Prediksi Klasifikasi")
st.write("Masukkan data fitur untuk memprediksi kelas menggunakan model yang telah dioptimasi.")

# Buat input fitur (ini disesuaikan dengan jumlah fitur X kamu)
# Misal jumlah fitur = 4 (seperti pada dataset Iris atau yang mirip)
feature_1 = st.number_input("Fitur 1", value=0.0)
feature_2 = st.number_input("Fitur 2", value=0.0)
feature_3 = st.number_input("Fitur 3", value=0.0)
feature_4 = st.number_input("Fitur 4", value=0.0)

# Tombol prediksi
if st.button("Prediksi"):
    # Ubah input menjadi array dan lakukan prediksi
    input_data = np.array([[feature_1, feature_2, feature_3, feature_4]])

    # Jika kamu menyimpan scaler, load dan transform dulu:
    # scaler = joblib.load('scaler.pkl')
    # input_data = scaler.transform(input_data)

    prediction = model.predict(input_data)

    st.success(f"Hasil Prediksi: {prediction[0]}")